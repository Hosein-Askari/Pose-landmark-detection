{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classification on a video.\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
    "\n",
    "\n",
    "# Open output video.\n",
    "out_video = cv2.VideoWriter(out_video_path, cv2.VideoWriter_fourcc(*'mp4v'), video_fps, (video_width, video_height))\n",
    "\n",
    "frame_idx = 0\n",
    "output_frame = None\n",
    "with tqdm.tqdm(total=video_n_frames, position=0, leave=True) as pbar:\n",
    "  while True:\n",
    "    # Get next frame of the video.\n",
    "    success, input_frame = video_cap.read()\n",
    "    if not success:\n",
    "      break\n",
    "\n",
    "    # Run pose tracker.\n",
    "    input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose_tracker.process(image=input_frame)\n",
    "    pose_landmarks = result.pose_landmarks\n",
    "\n",
    "    # Draw pose prediction.\n",
    "    output_frame = input_frame.copy()\n",
    "    if pose_landmarks is not None:\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=output_frame,\n",
    "          landmark_list=pose_landmarks,\n",
    "          connections=mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    if pose_landmarks is not None:\n",
    "      # Get landmarks.\n",
    "      frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]\n",
    "      pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]\n",
    "                                 for lmk in pose_landmarks.landmark], dtype=np.float32)\n",
    "      assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)\n",
    "\n",
    "      # Classify the pose on the current frame.\n",
    "      pose_classification = pose_classifier(pose_landmarks)\n",
    "\n",
    "      # Smooth classification using EMA.\n",
    "      pose_classification_filtered = pose_classification_filter(pose_classification)\n",
    "\n",
    "      # Count repetitions.\n",
    "      repetitions_count = repetition_counter(pose_classification_filtered)\n",
    "    else:\n",
    "      # No pose => no classification on current frame.\n",
    "      pose_classification = None\n",
    "\n",
    "      # Still add empty classification to the filter to maintaing correct\n",
    "      # smoothing for future frames.\n",
    "      pose_classification_filtered = pose_classification_filter(dict())\n",
    "      pose_classification_filtered = None\n",
    "\n",
    "      # Don't update the counter presuming that person is 'frozen'. Just\n",
    "      # take the latest repetitions count.\n",
    "      repetitions_count = repetition_counter.n_repeats\n",
    "\n",
    "    # Draw classification plot and repetition counter.\n",
    "    output_frame = pose_classification_visualizer(\n",
    "        frame=output_frame,\n",
    "        pose_classification=pose_classification,\n",
    "        pose_classification_filtered=pose_classification_filtered,\n",
    "        repetitions_count=repetitions_count)\n",
    "\n",
    "    # Save the output frame.\n",
    "    out_video.write(cv2.cvtColor(np.array(output_frame), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Show intermediate frames of the video to track progress.\n",
    "    if frame_idx % 50 == 0:\n",
    "      show_image(output_frame)\n",
    "\n",
    "    frame_idx += 1\n",
    "    pbar.update()\n",
    "\n",
    "# Close output video.\n",
    "out_video.release()\n",
    "\n",
    "# Release MediaPipe resources.\n",
    "pose_tracker.close()\n",
    "\n",
    "# Show the last frame of the video.\n",
    "if output_frame is not None:\n",
    "  show_image(output_frame)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
